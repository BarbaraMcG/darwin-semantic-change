# -*- coding: utf-8 -*-
"""Semantic_change_Darwin_BMcG.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jCQyTI8_wBtCVNteJOCNkLn8mRgK2SjZ

# Semantic change in Darwin letters
##Corpus processing of Darwin letters

Barbara McGillivray

This notebook contains the code for pre-processing the corpus of Darwin letters and train the embeddings. As some of the steps can take a long time to run, it is advisable to only run this notebook at the beginning of the project, to generate all the files and models needed for the subsequent script, "2_Semantic_change.ipynb".

##License note on data

We will use the letters from the Darwin Correspondence Project (https://www.darwinproject.ac.uk/). The letters have the following license statement: http://creativecommons.org/licenses/by-nc-nd/3.0/ . Many of the texts of the letters are still in copyright to the descendants of the authors. You cannot publish the derivatives of processing these letters nor the letters themselves.

##1. Initialisation

I install version 4.0 of gensim, which is needed to train the word2vec models.
"""

!pip install gensim==4.0

"""I install the language detection module:"""

!pip install langdetect

"""Download Spacy's English language model:"""

!python -m spacy download en

"""Import libraries"""

# Commented out IPython magic to ensure Python compatibility.
import os 
from bs4 import BeautifulSoup
from google.colab import drive
import csv
import numpy as np
import pandas as pd
from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer
from nltk.corpus import stopwords
import nltk
nltk.download('punkt')
nltk.download('stopwords')
import gensim
from scipy import spatial
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import time
from sklearn.decomposition import PCA
import spacy
import re
from statistics import mean
from langdetect import detect
# to make our plot outputs appear and be stored within the notebook:
# %matplotlib inline

"""I mount the Google drive folder. You should change the path to your Google Drive path:"""

# Commented out IPython magic to ensure Python compatibility.
drive.mount('/content/drive', force_remount = True)
# %ls '/content/drive/MyDrive/2021/Darwin paper/Files/'

"""##2. Reading the files

I unzip the file containing all the letters. 
**This only needs to be done once** and you can skip this cell.
"""

!unzip '/content/drive/MyDrive/2021/Darwin paper/Files/letters.zip' -d '/gdrive/My Drive/2021/Darwin paper/Files/Letters'

"""I define the name of the folder containing the unzipped files:"""

folder = '/content/drive/MyDrive/2021/Darwin paper/Files/letters/'

"""I define the list of all files:"""

files = os.listdir(folder)

"""How many files are in the folder?

"""

len(files)

"""We only want xml files:"""

files = [f for f in files[:] if f.endswith(".xml")]
len(files)

"""### Extracting the text of the letters

I create the output folder:
"""

out_folder = '/content/drive/My Drive/2021/Darwin paper/Files/Letters_text/'

"""We extract the trascribed text from each letter and save it in an output file.  **You can skip this cell.**"""

outfile = open(os.path.join(out_folder, 'transcription.csv'), 'w+')
output_writer = csv.writer(outfile, delimiter = "\t")

# Write header row:
output_writer.writerow(["File", "Year", "Sender", "Receiver", "Transcription"]) 

# I initialise the variable "count" to count the files that I'm reading one by one
count = 0

# I loop over each file in the "files" list
for fname in files:
  
  # The variable "count" is incremented
  count += 1
  print("Reading file number", str(count), fname)

  # the following two lines can be commented out if we want all files
  #if count > 200:
  #  break
    
  # I open each file
  with open(os.path.join(folder, fname), "r") as infile:

    # I read the content of the file in the variable "content"
    content = infile.read()

    # Initialize the fields required (sender, receiver, date_sent, keywords, abstract, letter_text):
    sender = ""
    receiver = ""
    date_sent = "" # the date in which the letter was sent
    year = ""
    letter_text = "" # transcription of the letter  

    # I parse the XML of the file with BeautifulSoup
    soup = BeautifulSoup(content,'xml')

    # extract sender, receiver and date_sent:

    corr_action = soup.find_all("correspAction")
    if corr_action:
      # loop over all notes under "correspAction":
      for s in corr_action:
          
        #extract receiver's name:
        if s.get('type') == "received":
          try:
            receiver = s.persName.get_text()
            #print("receiver:", receiver)
          except:
            print("No receiver")

        # extract senders' name:
        if s.get('type') == "sent":
          try:
            sender = s.persName.get_text()  
            #print("sender:", sender)
          except:
            print("No sender")
            
          # extract date sent:                  
          try:
            date_sent = s.date["when"]
            # the year is the first four characters of the date:
            year = date_sent[:4]
          except:
            print("No exact date")
            try:
              date_sent_not_before = s.date["notBefore"]
              # the year is the first four characters of the date:
              year_not_before = date_sent_not_before[:4]
            except:
              print("\tNo notBefore date")
            try:
              date_sent_not_after = s.date["notAfter"]
              year_not_after = date_sent_not_after[:4]
            except:
              print("\tNo notAfter date")
            if date_sent_not_before and date_sent_not_after:
              year = round(float((int(year_not_before)+int(year_not_after))/2))
            elif date_sent_not_before:
              year = year_not_before
            elif date_sent_not_after:
              year = year_not_after

      # extract the transcription of the letters:
      text = soup.find_all("div")
      # I loop over all children of nodes in "div":
      for s in text:
        # I save the text of the transcription in the variable "letter_text"
        if s.get('type') == "transcription":
          letter_text = s.get_text()  
          # Remove newlines and tabs:
          letter_text = letter_text.replace("\n", " ").replace("\r", " ").replace("\t", " ").replace("  ", " ")
          # Remove leading and trailing spaces:
          letter_text = letter_text.strip()
          # test print statement:
          #print("letter_text:", letter_text)
          
      if letter_text == "":
        # If the transcription is empty, I save the variable "letter_text" with the value "TRANSCRIPTION MISSING"
        letter_text = "TRANSCRIPTION MISSING"
        # test print statement:
        #print("Tag_text:", letter_text)

      # I only select those letters for which we have a year and that were sent by or received by Charles Darwin:
      if year is not "" and (sender == "Darwin, C. R." or receiver == "Darwin, C. R.") and letter_text is not "TRANSCRIPTION MISSING":
        # write to output file
        output_writer.writerow([fname, year, sender, receiver, letter_text])
      else:
        print("missing corr_action")                 
      
outfile.close()

"""I define a dataframe from the csv file:"""

df = pd.read_csv(os.path.join(out_folder, 'transcription.csv'), sep = "\t")

df.shape

"""Number of letters per year"""

df1 = df.groupby(['Year']).count()
df1 = df1['File']
df1

"""Visualize number of letters per year with a bar plot:"""

ax = df1.plot(kind='bar', figsize=(10,6), color="indigo", fontsize=11);
#ax.set_alpha(0.5)
ax.set_title("How many letters per year", fontsize=22)
plt.show()

"""## 3. Linguistic pre-processing

Add a space before and after every m-dash:
"""

s = "– – –Museum three — one from"
pat = re.compile(r"(\u2014)")
s = pat.sub(" \\1 ", s)
pat1 = re.compile(r"(\u2013)")
s = pat1.sub(" \\1 ", s)
s = s.replace("  ", " ")
s

s = "– – –Museum three — one from"
pat = re.compile(r"(\u2014)")
pat1 = re.compile(r"(\u2013)")
s = pat.sub(" \\1 ", pat1.sub(" \\1 ", s)).replace("  ", " ")
s

pat = re.compile(r"(\u2014)")
pat1 = re.compile(r"(\u2013)")
df['Transcription'] = [pat.sub(" \\1 ", pat1.sub(" \\1 ", df['Transcription'].iloc[i])).replace("  ", " ") for i in range(df.shape[0])]
df

df[df['File'] == "DCP-LETT-686.xml"]

"""load the spaCy language model:"""

sp = spacy.load("en_core_web_sm")

"""I split the letters into sentences, although this step may not be needed in the end:"""

#df['Transcription_sentences'] = [sp(df['Transcription'].iloc[i]) for i in range(df.shape[0])]
df['Transcription_spacy'] = [sp(df['Transcription'].iloc[i]) for i in range(df.shape[0])]
#df['Transcription_sentences'] = [ sentence for sentence in (df['Transcription'].iloc[i]).sents for i in range(df.shape[0])]
df

"""A token simply refers to an individual part of a sentence having some semantic value. Let's see what tokens we have in our document:"""

#df['Tokens'] = [[[word.text for word in sent] for sent in df['Transcription_spacy'].iloc[i].sents] for i in range(df.shape[0])]
#df['Tokens'] = [[word.text for word in sent for sent in df['Transcription_spacy'].iloc[i].sents] for i in range(df.shape[0])]
#df['Tokens'] = [[word.text for word in (df['Transcription_sentences'].iloc[i])] for i in range(df.shape[0])]
df['Tokens'] = [[word.text for word in sp(df['Transcription'].iloc[i])] for i in range(df.shape[0])]
df

df[df['File'] == "DCP-LETT-686.xml"]

"""### Lemmatisation"""

df['Lemmas'] = [[word.lemma_ for word in sp(df['Transcription'].iloc[i])] for i in range(df.shape[0])]
df

"""#### Stopwords exclusion and further filtering
I exclude non-alphabetical characters:
"""

df["Lemmas_clean"] = ''
for i in range(df.shape[0]):
  df['Lemmas_clean'].iloc[i] = [t for t in df['Lemmas'].iloc[i] if t not in '''!()-[]{};:\'"\,<>./?@#$%^&*_~\|–—\“’`''']# and t.isalpha() ]
df

"""I exclude stop words, although they will be needed for word embedding training:"""

stopWords = set(stopwords.words('english'))
df["Lemmas_nostop"] = ''
for i in range(df.shape[0]):
  df['Lemmas_nostop'].iloc[i] = [t for t in df['Lemmas'].iloc[i] if t not in stopWords]
df

"""I add the language of the letters in an additional column:"""

transcriptions = df[['Transcription']].to_numpy().flatten().tolist()
df[['language']] = [detect(x) for x in transcriptions]

df

"""How many letters per language?"""

df[['language']].value_counts()

df.shape

"""Only keep letters in English:"""

#df = df[~df.language.isin(['de', 'fr'])]
df = df[df.language.isin(['en'])]
df.shape

"""I save the dataframe to a file:"""

df.to_csv(os.path.join(out_folder, 'transcription_tokens.csv'), encoding='utf-8', index = False, sep = "\t")

"""I read the dataframe from file:"""

df = pd.read_csv(os.path.join(out_folder, 'transcription_tokens.csv'), sep = "\t")

df

"""## 4. Training word embeddings

Function for printing the vocabulary of a model
"""

def print_vocab(model, top_n = None):
  count = 0
  if top_n is not None:
    for index, word in enumerate(model.wv.index_to_key):
      count+= 1
      if count < top_n:
        print(f"word #{index}/{len(model.wv.index_to_key)} is {word}")
  else:
    for index, word in enumerate(model.wv.index_to_key):
      print(f"word #{index}/{len(model.wv.index_to_key)} is {word}")

pd.set_option('display.max_colwidth', -1)
(df['Lemmas_clean'])

"""I train Word2Vec embeddings from the letters. **You can skip this cell.**

`min_count`: the minimum frequency threshold allowed for a word to be included

`vector_size`: the number of dimensions in which we wish to represent our word. This is the size of the word embedding; typically between 100 and 1,000.

`window`: The size of the context window determines how many words before and after a given word would be included as context words of the given word.  Typically between 5 and 10.

`sg`: – Training algorithm: 1 for skip-gram; otherwise CBOW.

"""

start = time.time()
cbow_w5_f1 = gensim.models.Word2Vec(df['Lemmas_clean'], min_count=1, vector_size=300, window = 5, sg = 0)
end = time.time()
print("It has taken", round(end - start), "seconds")

print_vocab(cbow_w5_f1, 10)

"""Different parameters. **You can skip this cell.**"""

start = time.time()
cbow_w5_f0 = gensim.models.Word2Vec(df['Lemmas_clean'], min_count=0, vector_size=300, window = 5, sg = 0)
end = time.time()
print("cbow_w5_f0 has taken", round(end - start), "seconds")
start = time.time()
sg_w5_f1 = gensim.models.Word2Vec(df['Lemmas_clean'], min_count=1, vector_size=300, window = 5, sg = 1)
end = time.time()
print("sg_w5_f1 has taken", round(end - start), "seconds")
start = time.time()
sg_w5_f0 = gensim.models.Word2Vec(df['Lemmas_clean'], min_count=0, vector_size=300, window = 5, sg = 1)
end = time.time()
print("sg_w5_f0 has taken", round(end - start), "seconds")
start = time.time()
cbow_w10_f1 = gensim.models.Word2Vec(df['Lemmas_clean'], min_count=1, vector_size=300, window = 10, sg = 0)
end = time.time()
print("cbow_w10_f1 has taken", round(end - start), "seconds")
start = time.time()
cbow_w10_f0 = gensim.models.Word2Vec(df['Lemmas_clean'], min_count=0, vector_size=300, window = 10, sg = 0)
end = time.time()
print("cbow_w10_f0 has taken", round(end - start), "seconds")
start = time.time()
sg_w10_f1 = gensim.models.Word2Vec(df['Lemmas_clean'], min_count=1, vector_size=300, window = 10, sg = 1)
end = time.time()
print("sg_w10_f1 has taken", round(end - start), "seconds")
start = time.time()
sg_w10_f0 = gensim.models.Word2Vec(df['Lemmas_clean'], min_count=0, vector_size=300, window = 10, sg = 1)
end = time.time()
print("sg_w10_f0 has taken", round(end - start), "seconds")

"""Skip-gram is more computationally expensive, especially with larger window sizes.

Let's save the models so we can use them later. **You can skip this cell.**
"""

models_folder = '/content/drive/My Drive/2021/Darwin paper/Files/Models'
models = [cbow_w5_f1, cbow_w5_f0, sg_w5_f1, sg_w5_f0, cbow_w10_f1, cbow_w10_f0, sg_w10_f1, sg_w10_f0]
models_names = ["cbow_w5_f1", "cbow_w5_f0", "sg_w5_f1", "sg_w5_f0", "cbow_w10_f1", "cbow_w10_f0", "sg_w10_f1", "sg_w10_f0"]
for i in range(len(models)):
  print("Saving model number", i+1)
  models[i].save(os.path.join(models_folder, models_names[i]))

"""Load the models:"""

models_folder = '/content/drive/My Drive/2021/Darwin paper/Files/Models'
cbow_w5_f1 = gensim.models.Word2Vec.load(os.path.join(models_folder, "cbow_w5_f1"))
cbow_w5_f0 = gensim.models.Word2Vec.load(os.path.join(models_folder, "cbow_w5_f0"))
sg_w5_f1 = gensim.models.Word2Vec.load(os.path.join(models_folder, "sg_w5_f1"))
sg_w5_f0 = gensim.models.Word2Vec.load(os.path.join(models_folder, "sg_w5_f0"))
cbow_w10_f1 = gensim.models.Word2Vec.load(os.path.join(models_folder, "cbow_w10_f1"))
cbow_w10_f0 = gensim.models.Word2Vec.load(os.path.join(models_folder, "cbow_w10_f0"))
sg_w10_f1 = gensim.models.Word2Vec.load(os.path.join(models_folder, "sg_w10_f1"))
sg_w10_f0 = gensim.models.Word2Vec.load(os.path.join(models_folder, "sg_w10_f0"))
models = [cbow_w5_f1, cbow_w5_f0, sg_w5_f1, sg_w5_f0, cbow_w10_f1, cbow_w10_f0, sg_w10_f1, sg_w10_f0]
models_names = ["cbow_w5_f1", "cbow_w5_f0", "sg_w5_f1", "sg_w5_f0", "cbow_w10_f1", "cbow_w10_f0", "sg_w10_f1", "sg_w10_f0"]

"""Print the first ten words of the vocabulary of each model"""

for i in range(len(models)):
  print(models_names[i])
  print_vocab(models[i], 10)

"""Let's look at the most similar words (i.e. neighbours) of *letter*:"""

for i in range(len(models)):
  print(models_names[i], models[i].wv.similar_by_word('letter', 10))

"""#### Model choice
In order to do a systematic evaluation and choose the best model for our purposes, we would need to test them all against some "gold standard", e.g. a list of known synonyms for this corpus. This is the list suggested by Liz:
superfluous, unnecessary
Display, exhibit
Mimetic, imitative
Disappear, vanish
Alike, identical
"""

synonyms = dict({"superfluous": "unnecessary", "display": "exhibit", "mimetic" : "imitative", "disappear" : "vanish", "alike" : "identical"})
synonyms

"""Check that the synonym pairs are preserved by the models:"""

models_synonymity_average = dict()
for i in range(len(models)):
  print("Model", models_names[i])
  average_synonimity = 0
  synonymities = list()
  for s1 in synonyms:
    print("\tSimilarity between", s1, "and", synonyms[s1],  "in", models_names[i], models[i].wv.similarity(s1, synonyms[s1]))
    synonymities.append(models[i].wv.similarity(s1, synonyms[s1]))
  average_synonimity = mean(synonymities)
  print(average_synonimity)
  models_synonymity_average[models_names[i]] = average_synonimity
models_synonymity_average

"""I choose the model that maximises the average similarity between synonyms:"""

model_chosen_name = max(models_synonymity_average, key=models_synonymity_average.get)
for i in range(len(models)):
  if models_names[i] == model_chosen_name:
    models[i].save(os.path.join(models_folder, "Chosen_model"))
model_chosen_name

"""# Semantic change

Step 1: Define two time periods and split the corpus
"""

date = 1870
time_period_1 = list(range(min(df['Year']),date+1))
time_period_2 = list(range(date+1,max(df['Year'])+1))

df1 = df[df['Year'].isin(time_period_1)]
df2 = df[df['Year'].isin(time_period_2)]

"""Check the start and end dates:"""

max(df1['Year'])

min(df2['Year'])

"""Check that I haven't lost any line:"""

df.shape[0] == df1.shape[0] + df2.shape[0]

"""## Step 2. Train word embeddings for the two time periods

I train two word2vec models, one for each dataframe corresponding to a time period. I choose the parameters that led to the best model (see script 1_Corpus_Processing script). This takes approximately 5 minutes. **You can skip this cell.**
"""

model1 = gensim.models.Word2Vec(df1['Lemmas_clean'], min_count=1, vector_size=300, window = 5, sg = 1)
model2 = gensim.models.Word2Vec(df2['Lemmas_clean'], min_count=1, vector_size=300, window = 5, sg = 1)

"""Save the models. **You can skip this cell.**"""

models_folder = '/content/drive/My Drive/2021/Darwin paper/Files/Models'
models = [model1, model2]
models_names = ["model1", "model2"]
for i in range(len(models)):
  print("Saving model number", i+1)
  models[i].save(os.path.join(models_folder, models_names[i]))

"""I load the models:"""

models_folder = '/content/drive/My Drive/2021/Darwin paper/Files/Models'
model1 = gensim.models.Word2Vec.load(os.path.join(models_folder, "model1"))
model2 = gensim.models.Word2Vec.load(os.path.join(models_folder, "model2"))
models = [model1, model2]
models_names = ["model1", "model2"]

"""What are the vocabularies of these models?"""

print_vocab(model1, 10)

print_vocab(model2,10)

print(str(len(list(model1.wv.index_to_key))))
print(str(len(list(model2.wv.index_to_key))))

"""The intersection between the vocabularies of the two models:"""

vocab1 = set(list(model1.wv.index_to_key))
vocab2 = set(list(model2.wv.index_to_key))
len(vocab1.intersection(vocab2))

"""## Step 3. Embedding space alignment

I will be using code from https://gist.github.com/zhicongchen/9e23d5c3f1e5b1293b16133485cd17d8, ported from HistWords <https://github.com/williamleif/histwords>.

First, I define a function to find the intersection between the vocabularies of two word2vec models:
"""

def intersection_align_gensim(m1, m2, words=None):
    """
    Intersect two gensim word2vec models, m1 and m2.
    Only the shared vocabulary between them is kept.
    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.
    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).
    These indices correspond to the new syn0 and syn0norm objects in both gensim models:
        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0
        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2
    The .vocab dictionary is also updated for each model, preserving the count but updating the index.
    """

    # Get the vocab for each model
    vocab_m1 = set(m1.wv.index_to_key)
    vocab_m2 = set(m2.wv.index_to_key)

    # Find the common vocabulary
    common_vocab = vocab_m1 & vocab_m2
    if words: common_vocab &= set(words)

    # If no alignment necessary because vocab is identical...
    if not vocab_m1 - common_vocab and not vocab_m2 - common_vocab:
        return (m1,m2)

    # Otherwise sort by frequency (summed for both)
    common_vocab = list(common_vocab)
    common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, "count") + m2.wv.get_vecattr(w, "count"), reverse=True)
    # print(len(common_vocab))

    # Then for each model...
    for m in [m1, m2]:
        # Replace old syn0norm array with new one (with common vocab)
        indices = [m.wv.key_to_index[w] for w in common_vocab]
        old_arr = m.wv.vectors
        new_arr = np.array([old_arr[index] for index in indices])
        m.wv.vectors = new_arr

        # Replace old vocab dictionary with new one (with common vocab)
        # and old index2word with new one
        new_key_to_index = {}
        new_index_to_key = []
        for new_index, key in enumerate(common_vocab):
            new_key_to_index[key] = new_index
            new_index_to_key.append(key)
        m.wv.key_to_index = new_key_to_index
        m.wv.index_to_key = new_index_to_key
        
        print(len(m.wv.key_to_index), len(m.wv.vectors))
        
    return (m1,m2)

"""Then, I define a function for aligning two spaces with [Orthogonal Procrustes](https://simonensemble.github.io/2018-10/orthogonal-procrustes.html):"""

def smart_procrustes_align_gensim(base_embed, other_embed, words=None):
    """
    Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf
    Procrustes align two gensim word2vec models (to allow for comparison between same word across models).
    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.
        
    First, intersect the vocabularies (see `intersection_align_gensim` documentation).
    Then do the alignment on the other_embed model.
    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.
    Return other_embed.
    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).
    """

    # make sure vocabulary and indices are aligned
    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)

    # get the (normalized) embedding matrices
    base_vecs = in_base_embed.wv.get_normed_vectors()
    other_vecs = in_other_embed.wv.get_normed_vectors()

    # just a matrix dot product with numpy
    m = other_vecs.T.dot(base_vecs) 
    # SVD method from numpy
    u, _, v = np.linalg.svd(m)
    # another matrix operation
    ortho = u.dot(v) 
    # Replace original array with modified one, i.e. multiplying the embedding matrix by "ortho"
    other_embed.wv.vectors = (other_embed.wv.vectors).dot(ortho)    
    
    return other_embed

"""Now I can apply the function to my models:"""

smart_procrustes_align_gensim(model1, model2, words=None)

"""Now the two models have been aligned and have the same vocabulary:"""

len(model1.wv.index_to_key)

len(model2.wv.index_to_key)

model1.wv.index_to_key == model2.wv.index_to_key

"""I print the vocabulary:"""

model1.wv.index_to_key

"""I save the aligned models:"""

for i in range(len(models)):
  print(models_names[i])
  models[i].save(os.path.join(models_folder, models_names[i] + "_aligned"))

"""Let's look at the similarity between two words in the two spaces:"""

model1.wv.similarity("letter", "note")

model2.wv.similarity("letter", "note")

"""## Step 4: Measure change

Now I can measure the cosine similarity between the embedding of a word in the first time period and the embedding of the same word in the second time period.

Let's define a function that calculates the semantic change of a word:
"""

def semantic_change(word):
  sc = 1-spatial.distance.cosine(model1.wv[word], model2.wv[word])
  return sc

"""Now I calculate the semantic change for all words in the vocabulary, and store this in the dataframe semantic_change; I also add a column for the frequency of the word in the first space and another one for its frequency in the second space:"""

semantic_change_df = pd.DataFrame(([w, semantic_change(w), model1.wv.get_vecattr(w, "count") , model2.wv.get_vecattr(w, "count") ] for w in model1.wv.index_to_key), columns = ('Word', 'Semantic_change', "Frequency_t1", "Frequency_t2"))
semantic_change_df

"""Visualise the distribution of the semantic change scores with a histogram:"""

hist = semantic_change_df['Semantic_change'].hist()

"""Now I sort by decreasing semantic change score:"""

semantic_change_df_sorted = semantic_change_df.sort_values(by='Semantic_change', ascending=False)
semantic_change_df_sorted.head()

"""I add the total frequency:"""

semantic_change_df_sorted["Total_Frequency"] = semantic_change_df_sorted["Frequency_t1"]+semantic_change_df_sorted["Frequency_t2"]
semantic_change_df_sorted

"""I add a column with the category (function word, foreign word, numeral, and proper noun).

List of function words (from https://www.eltconcourse.com/training/initial/lexis/function_words.html)
"""

function_words = ['after', 'although', 'and', 'as', 'because', 'before', 'both', 'but', 'either', 'for', 'however', 'if','neither','nor','once','or','since','so','than','that','therefore','though','thus','till','unless','until','when','whenever','where','wherever','whether','while','yet	a','all','another','any','both','each','either','every','her','his','its','my','neither','no','other','our','per','some','that','the','their','these','this','those','whatever','whichever','your	about','above','across','after','against','ahead','along','among','amongst','around','as','at','bar','before','behind','below','beneath','beside','besides','between','beyond','but','by','down','during','except','for','from','in','inside','into','less','like	near','of','off','on','onto','opposite','outside','over','past','per','round','save','since','through','till','to','toward','under','underneath','until','up','upon','with','within','without	all','another','any','anybody','anyone','anything','both','each','either','everybody','everyone','everything','few','he','I','it','many','mine','neither','nobody','none','nothing','one','other','several','she','some','somebody','someone','something','that','these','they	this','those','we','what','whatever','which','whichever','who','whoever','whom','whose','you']

"""Applying the detect package doesn't work on individual words so I use the NLTK's English dictionary instead:"""

#semantic_change.df.sorted['Language'] = semantic_change.df.sorted['Word'].apply(detect) 
import nltk
nltk.download('words')
english_vocab = set(w.lower() for w in nltk.corpus.words.words())
len(english_vocab)

# create a list of our conditions
conditions = [
    (semantic_change_df_sorted['Word'].isin(function_words)), # function words
    ((semantic_change_df_sorted.Word.str.isdigit()) | (semantic_change_df_sorted['Word'].str.match('^\d*?1st$')) | (semantic_change_df_sorted['Word'].str.match('^\d*?2nd$')) | (semantic_change_df_sorted['Word'].str.match('^\d*?3rd$')) | (semantic_change_df_sorted['Word'].str.match('^\d*[^123]th$'))), # numbers
    (semantic_change_df_sorted['Word'].apply(lambda x: x[0].isupper() and x[1:len(x)].islower())), # proper nouns
    (np.invert(semantic_change_df_sorted['Word'].str.lower().isin(english_vocab))) # foreign words
    ]

# create a list of the values we want to assign for each condition
values = ['Function word','Numeral', 'Proper noun', 'Foreign word']

# create a new column and use np.select to assign values to it using our lists as arguments
semantic_change_df_sorted['Category_automatic'] = np.select(conditions, values, default = "")
semantic_change_df_sorted

semantic_change_df_sorted[semantic_change_df_sorted['Category_automatic'] == 'Function word']

"""I save this to a file:"""

output_folder = '/content/drive/MyDrive/2021/Darwin paper/Files/Semantic_change_output/'
if not os.path.exists(output_folder):
  os.makedirs(output_folder)
semantic_change_df_sorted.to_csv(os.path.join(output_folder, 'semantic_change_all_words.csv'), encoding='utf-8')

"""The top most changed words:"""

semantic_change_df_sorted.head(100)

"""I plot the terms by their total frequency and semantic change score:"""

ax = semantic_change.df.sorted.set_index('Semantic_change')['Total_Frequency'].plot(style='o')

"""Which words are the outliers? Function words and *Darwin*:"""

semantic_change_df_sorted.loc[semantic_change.df.sorted['Total_Frequency'] > 10000].Word.tolist()

"""I exclude the outliers:"""

semantic_change_df_sorted1 = semantic_change_df_sorted.loc[semantic_change_df_sorted['Total_Frequency'] < 10000]
semantic_change_df_sorted1.shape

"""Plot:"""

ax = semantic_change.df.sorted1.set_index('Semantic_change')['Total_Frequency'].plot(style='o')

"""What is the semantic change of the words we have chosen?"""

words_chosen = ['protoplasm', 'curious', 'fertilisation', 'fertilise', 'analogy', 'analogous', 'homology', 'homologous', 'cleistogamic', 'physiology', 'physiological', 'ferment', 'science', 'scientist', 'scientific', 'evolution', 'evolutionary', 'evolve']

semantic_change_df_sorted[semantic_change_df_sorted['Word'] == "fertilise"]

semantic_change_chosen = semantic_change_df_sorted[semantic_change_df_sorted['Word'].isin(words_chosen)]
semantic_change_chosen.to_csv(os.path.join(output_folder, 'semantic_change_chosen_words.csv'), encoding='utf-8')
semantic_change_chosen

min(semantic_change_chosen.Semantic_change)

min(semantic_change_chosen.Total_Frequency)

"""Now I focus on those words that have a frequency higher than min(semantic_change_chosen.Total_Frequency) and a semantic change score above min(semantic_change_chosen.Semantic_change). How many are there?"""

semantic_change_df_sorted_filtered = semantic_change_df_sorted1.loc[(semantic_change_df_sorted1['Total_Frequency'] > min(semantic_change_chosen.Total_Frequency)) & (semantic_change_df_sorted1['Semantic_change'] > min(semantic_change_chosen.Semantic_change))]
semantic_change_df_sorted_filtered.shape

"""Which words are these words?"""

semantic_change_df_sorted_filtered

"""Neighbours of *fertilisation* in the two time periods:"""

print(model1.wv.similar_by_word("fertilisation", 10))
print(model2.wv.similar_by_word("fertilisation", 10))
semantic_change_df_sorted_filtered[semantic_change_df_sorted_filtered['Word'] == "fertilisation"]

"""I create a list for these words:"""

highfreq_highlychanged_words = semantic_change_df_sorted_filtered.Word.tolist()
highfreq_highlychanged_words

"""Now I annotate the plot with these:"""

semantic_change_df_sorted_filtered_p = semantic_change_df_sorted_filtered[['Word','Semantic_change', 'Total_Frequency']]
fig, ax = plt.subplots()
semantic_change_df_sorted_filtered_p.plot('Semantic_change', 'Total_Frequency', kind='scatter', ax=ax)
for k, v in semantic_change_df_sorted_filtered_p.iterrows():
  word = v['Word']
  sc = v['Semantic_change']
  f = v['Total_Frequency']
  #print("w", v['Word'])
  #print("sc:", str(sc))
  #print("f:", str(f))
  ax.annotate(word, xy=(sc,f))

"""What are the neighbours of these words in the two spaces?"""

for word in highfreq_highlychanged_words:
  print(word)
  print("Neighbours in first space:")
  print(model1.wv.similar_by_word(word, 10))
  print("Neighbours in second space:")
  print(model2.wv.similar_by_word(word, 10))

"""I print this to an output file:"""

outfile = open(os.path.join(output_folder, 'Neighbours_changed_words.csv'), 'w+')

# Write header row:
outfile.write("Word, followed by  neighbours in first space and neighbours in second space\n") 
for word in highfreq_highlychanged_words:
  print(word)
  print("Neighbours in first space:")
  print(model1.wv.similar_by_word(word, 10))
  print("Neighbours in second space:")
  print(model2.wv.similar_by_word(word, 10))
  outfile.write(word+"\n")
  for (w,c) in model1.wv.similar_by_word(word, 10):
    outfile.write(w+"\t"+str(c))
  outfile.write("\n")
  for (w,c) in model2.wv.similar_by_word(word, 10):
    outfile.write(w+"\t"+str(c))
  outfile.write("\n")
  
outfile.close()

"""## An approach to semantic change detection using neighbours

We're going to implement the method proposed by Gonen, H., Jawahar, G., Seddah, D., & Goldberg, Y. (2020). Simple, Interpretable and Stable Method for Detecting Words with Usage Change across Corpora. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (pp. 538–555). https://doi.org/10.18653/v1/2020.acl-main.51

I extract all words with frequency at least 100 in both time periods:
"""

def extract_top100(model):
  vocab = model.wv.index_to_key
  vocab_100 = []
  for w in vocab:
    if model.wv.get_vecattr(w, "count") >= 100:
      vocab_100.append(w)
  return vocab_100

vocab1_100 = extract_top100(model1)
vocab2_100 = extract_top100(model2)
vocab_100 = list(set(vocab1_100) & set(vocab2_100))

"""How many words?"""

print(str(len(vocab_100)))

"""I extract the top 100 neighbours and save them in a dictionary mapping a word to the list of its 100 neighbours:"""

def extract_neighbours(w2vmodel, vocab_list):
  count = 0
  word2neighbours = dict()
  for w in vocab_list:
    count += 1
    if (count % 100) == 0:
      print(str(count), "out of", str(len(vocab_list)), w)
    neighbours = w2vmodel.wv.similar_by_word(w, 100)
    #print(str(neighbours))
    neighbour_list = list()
    for n in neighbours:
      neighbour_list.append(n[0])
    word2neighbours[w] = neighbour_list
  return word2neighbours

"""I apply this to model1 and model2 but for all words (not just those with frequency above 100):"""

word2neighbours1 = extract_neighbours(model1, model1.wv.index_to_key)
word2neighbours2 = extract_neighbours(model2, model2.wv.index_to_key)

"""For every word in the intersection between the two vcabularies, compare its neighbours in t1 and its neighbours in t2; the semantic change score is the number of shared neighbours between the two divided by 100.
NB This is different from the formula in Gonen et al. (2020), who take the negative of the overlap.
"""

vocab_12 = list(set(model1.wv.index_to_key) & set(model2.wv.index_to_key))
w2sc = dict()
for w in vocab_12:
  #sc = -len(list(set(word2neighbours1[w]) & set(word2neighbours2[w])))
  sc = len(list(set(word2neighbours1[w]) & set(word2neighbours2[w])))/100
  w2sc[w] = sc

"""I have saved these scores in the w2sc dictionary. Now I sort it by decreasing score to see which words changed the most:"""

w2sc_sorted = sorted(w2sc.items(), key=lambda kv: kv[1], reverse=False)
w2sc_sorted

"""What are the most changed words?"""

top_changed = [i[0] for i in w2sc_sorted][:50]
top_changed

"""What are their neighbours?"""

for w in top_changed:
  print(w)
  print(str(word2neighbours1[w]))
  print(str(word2neighbours2[w]))

semantic_change_neighbours_df = pd.DataFrame(([w, w2sc[w], model1.wv.get_vecattr(w, "count") , model2.wv.get_vecattr(w, "count") ] for w in vocab_12), columns = ('Word', 'Semantic_change_neighbour_score', "Frequency_t1", "Frequency_t2"))
semantic_change_neighbours_df.describe()

"""Visualise the distribution of the semantic change scores with a histogram:"""

hist = semantic_change_neighbours_df['Semantic_change_neighbour_score'].hist()

"""Now I sort by decreasing semantic change score:"""

semantic_change_neighbours_df_sorted = semantic_change_neighbours_df.sort_values(by='Semantic_change_neighbour_score', ascending=False)
semantic_change_neighbours_df_sorted.head()

"""I add the total frequency:"""

semantic_change_neighbours_df_sorted["Total_Frequency"] = semantic_change_neighbours_df_sorted["Frequency_t1"]+semantic_change_neighbours_df_sorted["Frequency_t2"]
semantic_change_neighbours_df_sorted

"""I add a column with the category (function word, foreign word, numeral, and proper noun)."""

# create a list of our conditions
conditions = [
    (semantic_change_neighbours_df_sorted['Word'].isin(function_words)), # function words
    ((semantic_change_neighbours_df_sorted.Word.str.isdigit()) | (semantic_change_neighbours_df_sorted['Word'].str.match('^\d*?1st$')) | (semantic_change_neighbours_df_sorted['Word'].str.match('^\d*?2nd$')) | (semantic_change_neighbours_df_sorted['Word'].str.match('^\d*?3rd$')) | (semantic_change_neighbours_df_sorted['Word'].str.match('^\d*[^123]th$'))), # numbers
    (semantic_change_neighbours_df_sorted['Word'].apply(lambda x: x[0].isupper() and x[1:len(x)].islower())), # proper nouns
    (np.invert(semantic_change_neighbours_df_sorted['Word'].str.lower().isin(english_vocab))) # foreign words
    ]

# create a list of the values we want to assign for each condition
values = ['Function word','Numeral', 'Proper noun', 'Foreign word']

# create a new column and use np.select to assign values to it using our lists as arguments
semantic_change_neighbours_df_sorted['Category_automatic'] = np.select(conditions, values, default = "")
semantic_change_neighbours_df_sorted

"""I save this to a file:"""

output_folder = '/content/drive/MyDrive/2021/Darwin paper/Files/Semantic_change_output/'
if not os.path.exists(output_folder):
  os.makedirs(output_folder)
semantic_change_neighbours_df_sorted.to_csv(os.path.join(output_folder, 'semantic_change_all_words_neighbourscore.csv'), encoding='utf-8')

"""## Comparison of the two methods for semantic change detection"""

semantic_change_neighbours_df_sorted.sort_values(by='Word', ascending=True)

semantic_change_df_sorted.sort_values(by='Word', ascending=True)

semantic_change_2methods = semantic_change_df_sorted.merge(semantic_change_neighbours_df_sorted, how = 'inner')
semantic_change_2methods

"""I integrate the neighbour-based semantic change score in the file:"""

semantic_change_2methods_all = semantic_change_df_sorted.merge(semantic_change_neighbours_df_sorted, how = 'outer')
semantic_change_2methods_all.to_csv(os.path.join(output_folder, 'semantic_change_all_words_2scores.csv'), encoding='utf-8')
semantic_change_2methods_all

"""I plot the terms by two semantic change scores:"""

ax = semantic_change_2methods.set_index('Semantic_change')['Semantic_change_neighbour_score'].plot(style='o')

"""There seems to be a positive correlation: the higher the cosine semantic change the higher the neighbour semantic change."""